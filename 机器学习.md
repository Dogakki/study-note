# 第二章 模型评估与选择

## 2.1 经验误差与过拟合

​		我们将学习器对样本的实际预测结果与样本的真实值之间的差异成为：**误差（error）**。定义：

- 在训练集上的误差称为**训练误差（training error）**或**经验误差（empirical error）**。
- 在测试集上的误差称为**测试误差（test error）**。
- 学习器在所有新样本上的误差称为**泛化误差**（generalization error）。

​		显然，我们希望得到的是在新样本上表现得很好的学习器，即泛化误差小的学习器。因此，我们应该让学习器尽可能地从训练集中学出普适性的“一般特征”，这样在遇到新样本时才能做出正确的判别。然而，当学习器把训练集学得“太好”的时候，即把一些训练样本的自身特点当做了普遍特征；同时也有学习能力不足的情况，即训练集的基本特征都没有学习出来。我们定义：

- 学习能力过强，以至于把训练样本所包含的不太一般的特性都学到了，称为：过拟合（overfitting）。
- 学习能太差，训练样本的一般性质尚未学好，称为：欠拟合（underfitting）。

​		可以得知：在过拟合问题中，训练误差十分小，但测试误差教大；在欠拟合问题中，训练误差和测试误差都比较大。目前，欠拟合问题比较容易克服，例如增加迭代次数等，但过拟合问题还没有十分好的解决方案，过拟合是机器学习面临的关键障碍。

![image-20231216161037893](E:\typora\Project\机器学习.assets\image-20231216161037893.png)

## 2.2 评估方法

​		在现实任务中，我们希望得到的是泛化误差小的学习器，理想的解决方案是对模型的泛化误差进行评估，然后选择泛化误差最小的那个学习器。但是，泛化误差指的是模型在所有新样本上的适用能力，我们无法直接获得泛化误差。

​		因此，通常我们采用一个“测试集”来测试学习器对新样本的判别能力，然后以“测试集”上的“测试误差”作为“泛化误差”的近似。显然：我们选取的测试集应尽可能与训练集互斥。训练集S和测试集T都是从数据集D中分出来的，以下有三种方式进行科学的分割。

### 2.2.1 留出法

​		将数据集D划分为两个互斥的集合，一个作为训练集S，一个作为测试集T，满足D=S∪T且S∩T=∅，常见的划分为：大约2/3-4/5的样本用作训练，剩下的用作测试。需要注意的是：**训练/测试集的划分要尽可能保持数据分布的一致性**，以避免由于分布的差异引入额外的偏差，常见的做法是采取分层抽样。同时，由于划分的随机性，单次的留出法结果往往不够稳定，一般要采用若干次随机划分，重复实验取平均值的做法。

### 2.2.2 交叉验证法

​		将数据集D划分为k个大小相同的互斥子集，满足D=D1∪D2∪...∪Dk，Di∩Dj=∅（i≠j），同样地尽可能保持数据分布的一致性，即采用分层抽样的方法获得这些子集。交叉验证法的思想是：每次用k-1个子集的并集作为训练集，余下的那个子集作为测试集，这样就有K种训练集/测试集划分的情况，从而可进行k次训练和测试，最终返回k次测试结果的均值。交叉验证法也称“k折交叉验证”，k最常用的取值是10，下图给出了10折交叉验证的示意图。

![image-20231216161824449](E:\typora\Project\机器学习.assets\image-20231216161824449.png)

​		与留出法类似，将数据集D划分为K个子集的过程具有随机性，因此K折交叉验证通常也要重复p次，称为p次k折交叉验证，常见的是10次10折交叉验证，即进行了100次训练/测试。特殊地当划分的k个子集的每个子集中只有一个样本时，称为“留一法”，显然，留一法的评估结果比较准确，但对计算机的消耗也是巨大。



### 2.2.3 自助法

​		我们希望评估的是用整个D训练出的模型。但在留出法和交叉验证法中，由于保留了一部分样本用于测试，因此实际评估的模型所使用的训练集比D小，这必然会引入一些因训练样本规模不同而导致的估计偏差。留一法受训练样本规模变化的影响较小，但计算复杂度又太高了。“自助法”正是解决了这样的问题。

​		**自助法的基本思想是：**                                                                                                                                                                 		给定包含m个样本的数据集D，每次随机从D 中挑选一个样本，将其拷贝放入D'，然后再将该样本放回初始数据集D 中，使得该样本在下次采样时仍有可能被采到。重复执行m 次，就可以得到了包含m个样本的数据集D'。可以得知在m次采样中，样本始终不被采到的概率取极限为：

![Image Name](https://cdn.kesci.com/upload/image/q61ku99nik.png?imageView2/0/w/960/h/960)

​		这样，通过自助采样，初始样本集D中大约有36.8%的样本没有出现在D'中，于是可以将D'作为训练集，D-D'作为测试集。自助法在数据集较小，难以有效划分训练集/测试集时很有用，但由于自助法产生的数据集（随机抽样）改变了初始数据集的分布，因此引入了估计偏差。在初始数据集足够时，留出法和交叉验证法更加常用。

## 

### 2.2.4 调参与最终模型

调参：在进行模型评估和选择时，除了要对使用学习算法进行选择，还需要对算法参数进行设定。

学习算法的很多参数是在实数范围内取值，因此，对每种参数取值都训练出模型来是不可行的。常用的做法是：对每个参数选定一个范围和步长λ，这样使得学习的过程变得可行



## 2.3 性能度量

性能度量：衡量模型泛化能力的评价标准。

​		性能度量反映了任务需求，在对比不同模型的能力时，使用不同的性能度量往往会导致不同的评判结果，这意味着一个模型的好坏不仅取决于算法和数据，还取决于任务需求。

​		在预测任务中，给定样例集
$$
D={(x_1,y_1),(x_2,y_2),...,(x_m,y_m)}
$$
​		其中y是x的真实标记，要评估学习器f的性能，就要把学习器预测结果f(x)与真实标记y进行比较。

​		回归任务最常用的性能度量是均方误差

![Image Name](https://cdn.kesci.com/upload/image/q61kxmin0z.png?imageView2/0/w/960/h/960)

### 2.3.1 错误率与精度

​		错误率与精度是最常用的两种性能度量，既适合与二分类任务，也适合与多分类任务。

​												![Image Name](https://cdn.kesci.com/upload/image/q61ky8r30d.png?imageView2/0/w/960/h/960)

### 2.3.2 查准率、查全率与F1

​		很多时候，错误率与精度不能够满足所有任务需求。例如：在推荐系统中，我们只关心推送给用户的内容用户是否感兴趣（即查准率），或者说所有用户感兴趣的内容我们推送出来了多少（即查全率）。因此，使用查准/查全率更适合描述这类问题。对于二分类问题，分类结果混淆矩阵与查准/查全率定义如下：

<img src="https://cdn.kesci.com/upload/image/q61kzc7otc.png?imageView2/0/w/960/h/960" alt="Image Name"  />

​		P（查准率）即模型预测出的正例有哪些是真正的正例，R（查全率）即所有正例有哪些被预测出来了。

​		查准率与查全率是一对矛盾的度量，一般来说，查准率高时，查全率往往偏低；查全率高时，查准率就会低。以查准率为纵轴，查全率为横轴作图，就得到了查准率、查全率曲线，简称“P-R曲线”，显示该曲线的图为“P-R图”

![Image Name](https://cdn.kesci.com/upload/image/q61l0t71mv.png?imageView2/0/w/960/h/960)

​		在比较时，若一个学习器的P-R曲线完全被另一个P-R曲线包住，则证明后者的性能优于前者，例如A优于C

​		若两条曲线有交叉，例如A和B，则只能在具体的查准率和查全率下进行比较。

​		平衡点（Break-Even Point 简称BEP）是查准率＝查全率的取值,A的BEP值大于B的BEP值，则可以证明学习器A大于B。



​		P和R指标有时会出现矛盾的情况，这样就需要综合考虑他们，最常见的方法就是F-Measure，又称F-Score。F-Measure是P和R的加权调和平均，即：

​		                                                              ![Image Name](https://cdn.kesci.com/upload/image/q61l1di7g7.png?imageView2/0/w/960/h/960)

​		特别地，当β=1时，也就是常见的F1度量，是P和R的调和平均，当F1较高时，模型的性能越好。

![Image Name](https://cdn.kesci.com/upload/image/q61l2khyoy.png?imageView2/0/w/960/h/960)

​		β>0度量了查全率对查准率的相对重要性。β=1时退化为标准的F1；β>1时查全率有更大的影响；β<1时，查准率有更大的影响。

​		有时候我们会有多个二分类混淆矩阵，例如：多次训练或者在多个数据集上训练，那么估算全局性能的方法有两种，分为宏观和微观。简单理解，宏观就是先算出每个混淆矩阵的P值和R值，然后取得平均P值macro-P和平均R值macro-R，在算出Fβ或F1，而微观则是计算出混淆矩阵的平均TP、FP、TN、FN，接着进行计算P、R，进而求出Fβ或F1。

![Image Name](https://cdn.kesci.com/upload/image/q61l35d0e5.png?imageView2/0/w/960/h/960)

### 2.3.3 ROC与AUC

​		如上所述：学习器对测试样本的评估结果一般为一个实值或概率，设定一个阈值，大于阈值为正例，小于阈值为负例，因此这个实值的好坏直接决定了学习器的泛化性能，若将这些实值排序，则排序的好坏决定了学习器的性能高低。ROC曲线正是从这个角度出发来研究学习器的泛化性能，ROC曲线与P-R曲线十分类似，都是按照排序的顺序逐一按照正例预测，不同的是ROC曲线以“真正例率”（True Positive Rate，简称TPR）为横轴，纵轴为“假正例率”（False Positive Rate，简称FPR），ROC偏重研究基于测试样本评估值的排序好坏。

![Image Name](https://cdn.kesci.com/upload/image/q61l4ldd4k.png?imageView2/0/w/960/h/960)

​		与P-R线类似，如果一条ROC将另一条ROC完全包住，则证明后者的性能强于前者。若ROC曲线发生交叉，则无法直接进行判断，较为合理地方式是比较ROC曲线下面的面积，即AUC。

​		AUC可以通过对ROC曲线求面积得出

![Image Name](https://cdn.kesci.com/upload/image/q61l60ltdz.png?imageView2/0/w/960/h/960)

### 2.3.4 代价敏感错误率与代价曲线、

​		不同类型的错误所造成的后果不同，为了权衡不同类型的错误所造成的的损失，可以为错误赋予“非均等代价”。

​		以二分任务为例，我们可以根据任务的领域知识设定一个“代价”矩阵

![Image Name](https://cdn.kesci.com/upload/image/q61lgwniqb.png?imageView2/0/w/960/h/960)

​		非均等错误代价下，我们希望的是最小化“总体代价”，这样“代价敏感”的错误率为：

![Image Name](https://cdn.kesci.com/upload/image/q61lhg5rx1.png?imageView2/0/w/960/h/960)

​		同样对于ROC曲线，在非均等错误代价下，演变成了“代价曲线”，代价曲线横轴是取值在[0,1]之间的正例概率代价，式中p表示正例的概率，纵轴是取值为[0,1]的归一化代价。

<img src="https://cdn.kesci.com/upload/image/q61lhyqcjq.png?imageView2/0/w/960/h/960" alt="Image Name" style="zoom:120%;" />

​									![Image Name](https://cdn.kesci.com/upload/image/q61lia9gex.png?imageView2/0/w/960/h/960)

​		代价曲线的绘制很简单：设ROC曲线上一点的坐标为(TPR，FPR) ，则可相应计算出FNR，然后在代价平面上绘制一条从(0，FPR) 到(1，FNR) 的线段，线段下的面积即表示了该条件下的期望总体代价；如此将ROC 曲线土的每个点转化为代价平面上的一条线段，然后取所有线段的下界，围成的面积即为在所有条件下学习器的期望总体代价，如图所示：

![Image Name](https://cdn.kesci.com/upload/image/q61lim3jzc.png?imageView2/0/w/960/h/960)

## 2.4 比较检验